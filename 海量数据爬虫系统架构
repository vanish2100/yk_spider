
资源管理、反监控管理、抓取管理、监控管理
资源管理指网站分类体系、网站、网站访问url等基本资源的管理维护
反监控管理指被访问网站（特别是社会化媒体）会禁止爬虫访问，怎么让他们不能监控到我们的访问时爬虫软件，这就是反监控机制了；
监控管理指不管什么系统都可能出问题，如果对方服务器宕机、网页改版、更换地址等我们需要第一时间知道，这时监控系统就起到出现了问题及时发现并通知联系人。
一定时间内单IP访问次数
一定时间内单账号访问次数

抓取管理指通过url，结合资源、反监控抓取数据并存储；我们现在大部分爬虫系统，很多都需要自己设定正则表达式，或者使用htmlparser、jsoup等软件来硬编码解决结构化抓取的问题。

1、有些网站利用js生成网页内容，直接查看源代码是一堆js。 可以使用mozilla、webkit等可以解析浏览器的工具包解析js、ajax，不过速度会有点慢。
2、网页里有一些css隐藏的文字。使用工具包把css隐藏文字去掉。
3、图片flash信息。 如果是图片中文字识别，这个比较好处理，能够使用ocr识别文字就行，如果是flash目前只能存储整个url。
4、一个网页有多个网页结构。如果只有一套抓取规则肯定不行的，需要多个规则配合抓取。
5、html不完整，不完整就不能按照正常模式去扣取。这个时候用xpath肯定解析不了，我们可以先用htmlcleaner清洗网页后再解析。
6、 如果网站多起来，规则配置这个工作量也会非常大。如何帮助系统快速生成规则呢？首先可以配置规则可以通过可视化配置，比如用户在看到的网页想对它抓取数据，只需要拉开插件点击需要的地方，规则就自动生成好了。另在量比较大的时候可视化还是不够的，可以先将类型相同的网站归类，再通过抓取的一些内容聚类，可以统计学、可视化抓取把内容扣取出几个版本给用户去纠正，最后确认的规则就是新网站的规则。这些算法后续再讲。
7、对付重复的网页，如果重复抓取会浪费资源，如果不抓需要一个海量的去重判断缓存。判断抓不抓，抓了后存不存，并且这个缓存需要快速读写。常见的做法有bloomfilter、相似度聚合、分类海明距离判断。

反扒方法
验证码破解、代理切换、自动登录

搜索系统包括三大模块：数据采集模块、页面清洗模块、数据库模块

分布式爬虫：Nutch，主要依赖Hadoop，存储于Mysql，Hbase，Hdfs等。

WebMagic ，若分布式，需要集成插件 webmagic-extension，通过redis来存储URL。
webmagic的架构和设计参考了以下两个项目，感谢以下两个项目的作者：
python爬虫  scrapy  https://github.com/scrapy/scrapy
Java爬虫    Spiderman  https://gitcafe.com/laiweiwei/Spiderman
webmagic主要包括两个包：
webmagic-core
webmagic核心部分，只包含爬虫基本模块和基本抽取器。webmagic-core的目标是成为网页爬虫的一个教科书般的实现。
webmagic-extension
webmagic的扩展模块，提供一些更方便的编写爬虫的工具。包括注解格式定义爬虫、JSON、分布式等支持。
webmagic还包含两个可用的扩展包，因为这两个包都依赖了比较重量级的工具，所以从主要包中抽离出来，这些包需要下载源码后自己编译：
webmagic-saxon
webmagic与Saxon结合的模块。Saxon是一个XPath、XSLT的解析工具，webmagic依赖Saxon来进行XPath2.0语法解析支持。
webmagic-selenium
webmagic与Selenium结合的模块。Selenium是一个模拟浏览器进行页面渲染的工具，webmagic依赖Selenium进行动态页面的抓取。



单机爬虫：Crawler4j、WebMagic、WebCollector


抓取策略有两种：
1、深度优先的抓取策略
2.广度（宽度）优先的抓取策略

网络爬虫抓取步骤（流程）:
1.初始化根（入口）URL
2.判断是否满足URL抓取终止条件（如果URL队列当中的元素为空，或者抓取到了指定数量），如果满足，退出抓取程序，如果不满足，进行3
3.取出最新的URL(URL出列)
4.将此URL对应的网页通过一个网页下载器下载到本地
5.抽取此网页满足条件的URL，并添加到URL队列当中，返回第2步

Scrapy主要组件
1、引擎(Scrapy): 用来处理整个系统的数据流处理, 触发事务(框架核心)。
2、调度器(Scheduler): 用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址。
3、下载器(Downloader): 用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)。
4、爬虫(Spiders): 爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面。
5、项目管道(Pipeline): 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
6、下载器中间件(Downloader Middlewares): 位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
7、爬虫中间件(Spider Middlewares): 介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
8、调度中间件(Scheduler Middewares): 介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。





